# -*- coding: utf-8 -*-
import os, re, unicodedata, datetime
import numpy as np
import pytz
import streamlit as st
import chromadb
from chromadb.config import Settings
from dotenv import load_dotenv
from serpapi import GoogleSearch
import base64 

from langchain_openai import AzureChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from langchain_core.tools import tool
from langchain_core.messages.tool import ToolMessage
from embeddings import get_embedding
# ========= ENV & GLOBALS =========
load_dotenv()

DB_PATH = os.getenv("DB_PATH", ".chroma_db")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "ptit_giaotrinh_2")
FETCH_K = int(os.getenv("FETCH_K", "30"))
TOP_K   = int(os.getenv("TOP_K", "10"))
DIST_THRES = float(os.getenv("DIST_THRES", "1.60"))
ROUTE_MIN_COS = float(os.getenv("ROUTE_MIN_COS", "0.30"))

# Optional reranker (n·∫øu kh√¥ng c√≥ file rerank.py th√¨ b·ªè qua)

# ========= TOOLS (cho chitchat) =========
@tool
def get_current_time(timezone: str = "Asia/Ho_Chi_Minh") -> str:
    """Tr·∫£ v·ªÅ th·ªùi gian hi·ªán t·∫°i (ti·∫øng Vi·ªát)."""
    try:
        tz = pytz.timezone(timezone)
        now = datetime.datetime.now(tz)
        weekdays = {
            'Monday': 'Th·ª© Hai', 'Tuesday': 'Th·ª© Ba', 'Wednesday': 'Th·ª© T∆∞',
            'Thursday': 'Th·ª© NƒÉm', 'Friday': 'Th·ª© S√°u',
            'Saturday': 'Th·ª© B·∫£y', 'Sunday': 'Ch·ªß Nh·∫≠t'
        }
        wd_vi = weekdays.get(now.strftime('%A'), 'Th·ª©')
        date_vi = now.strftime(f'{wd_vi}, ng√†y %d th√°ng %m nƒÉm %Y')
        time_vi = now.strftime('%H:%M:%S')
        return f"Th·ªùi gian hi·ªán t·∫°i ·ªü {timezone} l√† {time_vi} v√†o {date_vi}."
    except Exception as e:
        return f"L·ªói khi l·∫•y th·ªùi gian hi·ªán t·∫°i: {e}"

@tool
def google_search(query):
    """
    S·ª≠ d·ª•ng khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ th√¥ng tin chung, s·ª± ki·ªán, ƒë·ªãnh nghƒ©a,
    ho·∫∑c b·∫•t c·ª© ƒëi·ªÅu g√¨ kh√¥ng thu·ªôc 5 m√¥n h·ªçc ch√≠nh (HƒêH, PTTK, TTHCM, XLA, LTDƒê)
    v√† c≈©ng kh√¥ng ph·∫£i l√† h·ªèi gi·ªù ho·∫∑c th·ªùi ti·∫øt.
    """
    try:
        api_key = os.getenv("SERPAPI_API_KEY")
        params = {
            "q":query,
            "api_key":api_key,
            "location": 'Vietnam',
            "gl": 'vn',
            'hl': 'vi'
        }
        search = GoogleSearch(params)
        results = search.get_dict()
        snippets = []
        if "organic_results" in results:
            for res in results["organic_results"][:3]:
                if "snippet" in res:
                    snippets.append(res["snippet"])
        if not snippets:
            return f"Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ cho truy v·∫•n: {query}"

        return "D∆∞·ªõi ƒë√¢y l√† c√°c k·∫øt qu·∫£ t√≥m t·∫Øt t√¨m ƒë∆∞·ª£c:\n- " + "\n- ".join(snippets)
    except Exception as e:
        return f"L·ªói khi th·ª±c hi·ªán t√¨m ki·∫øm Google: {e}"

GENERAL_TOOLS = [get_current_time,google_search]

# ========= ROUTER & HELPERS =========
def vn_normalize(s):
    if not s: return ""
    s = unicodedata.normalize("NFC", s)
    return re.sub(r"\s+", " ", s).strip()

ROUTE_SAMPLES = {
    "hedieuhanh": [
        "H·ªá ƒëi·ªÅu h√†nh l√† g√¨?",
        "M√¥n h·ªá ƒëi·ªÅu h√†nh c√≥ m·∫•y ch∆∞∆°ng?",
        "B·∫°n c√≥ th·ªÉ gi·∫£i th√≠ch v·ªÅ deadlock trong h·ªá ƒëi·ªÅu h√†nh kh√¥ng?",
        "S·ª± kh√°c bi·ªát gi·ªØa ti·∫øn tr√¨nh (process) v√† lu·ªìng (thread) l√† g√¨?",
        "B·ªô nh·ªõ ·∫£o (virtual memory) ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?",
        "C√°c gi·∫£i thu·∫≠t l·∫≠p l·ªãch CPU ph·ªï bi·∫øn l√† g√¨?",
        "So s√°nh FCFS v√† Round Robin.",
        "Th√¥ng tin v·ªÅ m√¥n h·ªçc H·ªá ƒëi·ªÅu h√†nh.",
        "Kernel l√† g√¨?",
        "H·ªá th·ªëng file (file system) l√† g√¨?",
        "Th√¥ng tin v·ªÅ m√¥n H·ªá ƒëi·ªÅu h√†nh PTIT?",
    ],
    "Phantichthietkehttt": [
        "M√¥n ph√¢n t√≠ch thi·∫øt k·∫ø h·ªá th·ªëng th√¥ng tin c√≥ m·∫•y ch∆∞∆°ng?",
        "Bi·ªÉu ƒë·ªì Use Case (Use Case Diagram) d√πng ƒë·ªÉ l√†m g√¨?",
        "H√£y gi·∫£i th√≠ch v·ªÅ bi·ªÉu ƒë·ªì l·ªõp (Class Diagram).",
        "S·ª± kh√°c bi·ªát gi·ªØa y√™u c·∫ßu ch·ª©c nƒÉng v√† y√™u c·∫ßu phi ch·ª©c nƒÉng l√† g√¨?",
        "UML l√† g√¨ v√† n√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng nh∆∞ th·∫ø n√†o?",
        "So s√°nh c√°c m·ªëi quan h·ªá: Association, Aggregation, v√† Composition.",
        "H√£y k·ªÉ t√™n m·ªôt v√†i m√¥ h√¨nh ph√°t tri·ªÉn ph·∫ßn m·ªÅm.",
        "M√¥ h√¨nh Agile l√† g√¨?",
        "M√¥ h√¨nh th√°c n∆∞·ªõc (Waterfall) ho·∫°t ƒë·ªông ra sao?",
        "DAO (Data Access Object) l√† g√¨?",
        "Th√¥ng tin v·ªÅ m√¥n Ph√¢n t√≠ch thi·∫øt k·∫ø h·ªá th·ªëng th√¥ng tin."
    ],
    "tutuonghcm": [
        "M√¥n t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh c√≥ m·∫•y ch∆∞∆°ng?",
        "Ngu·ªìn g·ªëc c·ªßa T∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh l√† g√¨?",
        "N·ªôi dung c·ªët l√µi c·ªßa T∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh v·ªÅ ch·ªß nghƒ©a x√£ h·ªôi l√† g√¨?",
        "T∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh v·ªÅ con ƒë∆∞·ªùng c√°ch m·∫°ng gi·∫£i ph√≥ng d√¢n t·ªôc l√† g√¨?",
        "Vai tr√≤ c·ªßa ƒê·∫£ng C·ªông s·∫£n Vi·ªát Nam theo t∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh?",
        "B·∫°n c√≥ th·ªÉ t√≥m t·∫Øt c√°c ƒë·∫∑c tr∆∞ng c·ªßa ch·ªß nghƒ©a x√£ h·ªôi ·ªü Vi·ªát Nam kh√¥ng?",
        "Ch·ªß nghƒ©a M√°c-L√™nin c√≥ vai tr√≤ g√¨ trong T∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh?",
        "Th√¥ng tin v·ªÅ m√¥n T∆∞ t∆∞·ªüng H·ªì Ch√≠ Minh.",
        "S·ª± ki·ªán l·ªãch s·ª≠ 17h30 ng√†y 7/5/1954 l√† g√¨?",
        "C√¥ Ph·∫°m Th·ªã Kh√°nh l√† ai?"
    ],
    "Xulyanh": [
        "M√¥ h√¨nh m√†u l√† g√¨?"
        "M√¥n x·ª≠ l√Ω ·∫£nh c√≥ m·∫•y ch∆∞∆°ng?",
        "X·ª≠ l√Ω ·∫£nh (image processing) l√† g√¨?",
        "Ph√©p l·ªçc Gaussian (Gaussian filter) ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ l√†m g√¨?",
        "Ph√¢n ƒëo·∫°n ·∫£nh (image segmentation) l√† g√¨?",
        "Bi·∫øn ƒë·ªïi Fourier trong x·ª≠ l√Ω ·∫£nh c√≥ √Ω nghƒ©a g√¨?",
        "So s√°nh ph√©p to√°n h√¨nh th√°i h·ªçc Erosion (co) v√† Dilation (gi√£n).",
        "Thresholding (ng∆∞·ª°ng h√≥a) trong x·ª≠ l√Ω ·∫£nh l√† g√¨?",
        "L√†m th·∫ø n√†o ƒë·ªÉ ph√°t hi·ªán bi√™n (edge detection) trong m·ªôt b·ª©c ·∫£nh?",
        "B·ªô l·ªçc Median (Median filter) kh√°c g√¨ b·ªô l·ªçc trung b√¨nh (Mean filter)?",
        "Th√¥ng tin v·ªÅ m√¥n X·ª≠ l√Ω ·∫£nh PTIT"
    ],
    "chitchat": [
        "Th·ªùi ti·∫øt h√¥m nay nh∆∞ th·∫ø n√†o?",
        "Ngo√†i tr·ªùi n√≥ng bao nhi√™u?",
        "Ng√†y mai c√≥ m∆∞a kh√¥ng?",
        "Nhi·ªát ƒë·ªô hi·ªán t·∫°i l√† bao nhi√™u?",
        "B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt ƒëi·ªÅu ki·ªán th·ªùi ti·∫øt hi·ªán t·∫°i kh√¥ng?",
        "Cu·ªëi tu·∫ßn n√†y c√≥ n·∫Øng kh√¥ng?",
        "Nhi·ªát ƒë·ªô h√¥m qua l√† bao nhi√™u?",
        "ƒê√™m nay tr·ªùi s·∫Ω l·∫°nh ƒë·∫øn m·ª©c n√†o?",
        "Ai l√† t·ªïng th·ªëng ƒë·∫ßu ti√™n c·ªßa Hoa K·ª≥?",
        "Chi·∫øn tranh th·∫ø gi·ªõi th·ª© hai k·∫øt th√∫c v√†o nƒÉm n√†o?",
        "B·∫°n c√≥ th·ªÉ k·ªÉ cho t√¥i v·ªÅ l·ªãch s·ª≠ c·ªßa internet kh√¥ng?",
        "Th√°p Eiffel ƒë∆∞·ª£c x√¢y d·ª±ng v√†o nƒÉm n√†o?",
        "Ai ƒë√£ ph√°t minh ra ƒëi·ªán tho·∫°i?",
        "T√™n c·ªßa b·∫°n l√† g√¨?",
        "B·∫°n c√≥ t√™n kh√¥ng?",
        "T√¥i n√™n g·ªçi b·∫°n l√† g√¨?",
        "Ai ƒë√£ t·∫°o ra b·∫°n?",
        "B·∫°n bao nhi√™u tu·ªïi?",
        "B·∫°n c√≥ th·ªÉ k·ªÉ cho t√¥i m·ªôt s·ª± th·∫≠t th√∫ v·ªã kh√¥ng?",
        "B·∫°n c√≥ bi·∫øt b·∫•t k·ª≥ c√¢u ƒë·ªë th√∫ v·ªã n√†o kh√¥ng?",
        "M√†u s·∫Øc y√™u th√≠ch c·ªßa b·∫°n l√† g√¨?",
        "B·ªô phim y√™u th√≠ch c·ªßa b·∫°n l√† g√¨?",
        "B·∫°n c√≥ s·ªü th√≠ch n√†o kh√¥ng?",
        "√ù nghƒ©a c·ªßa cu·ªôc s·ªëng l√† g√¨?",
        "B·∫°n c√≥ th·ªÉ k·ªÉ cho t√¥i m·ªôt c√¢u chuy·ªán c∆∞·ªùi kh√¥ng?",
        "Th·ªß ƒë√¥ c·ªßa Ph√°p l√† g√¨?",
        "D√¢n s·ªë th·∫ø gi·ªõi l√† bao nhi√™u?",
        "C√≥ bao nhi√™u ch√¢u l·ª•c?",
        "Ai ƒë√£ vi·∫øt 'Gi·∫øt con chim nh·∫°i'?",
        "B·∫°n c√≥ th·ªÉ cho t√¥i m·ªôt c√¢u n√≥i c·ªßa Albert Einstein kh√¥ng?",
        "T√≥m t·∫Øt n·ªôi dung phim M∆∞a ƒê·ªè c·ªßa ƒë·∫°o di·ªÖn ƒê·∫∑ng Th√°i Huy·ªÅn?",
        "Anh T·∫° l√† ai?",
        "MH370 l√† c√°i g√¨?",
        "Nh·ª° th√≠ch a Quang l√≠nh VNCH th√¨ sao?",
        "C√¥ ƒê√†o Th·ªã Th√∫y Qu·ª≥nh l√† ai?",
        "Th·∫ßy ƒê·∫∑ng Ho√†ng Long l√† ai?",
        "Th·∫ßy Nguy·ªÖn M·∫°nh H√πng l√† ai?",
        "C√¥ ƒê·ªó Th·ªã B√≠ch Ng·ªçc l√† ai?",
        "Th·∫ßy Ph·∫°m VƒÉn C∆∞·ªùng l√† ai?",
        "T·ªïng th·ªëng Nga l√† ai?"
    ]
}


@st.cache_resource
def load_embedder():
    return get_embedding()

@st.cache_resource
def load_reranker():
    try:
        from rerank import Reranker
        return Reranker()
    except Exception:
        return None 
@st.cache_resource
def load_llm():
    """Load LLM (ch·ªâ 1 l·∫ßn)."""
    return AzureChatOpenAI(
        deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
        api_key=os.getenv("AZURE_OPENAI_KEY"),
        api_version=os.getenv("AZURE_OPENAI_VERSION")
    )

@st.cache_resource
def load_db_collection():
    """Load ChromaDB collection (ch·ªâ 1 l·∫ßn)."""
    client = chromadb.PersistentClient(path=DB_PATH, settings=Settings(anonymized_telemetry=False))
    collection = client.get_collection(name=COLLECTION_NAME)
    return collection

embedder = load_embedder()
reranker = load_reranker()
llm = load_llm()
collection = load_db_collection()

def l2_norm(x):
    n = np.linalg.norm(x, axis=1, keepdims=True)
    return x / (n + 1e-12)

def build_route_index(embedder):
    idx = {}
    for route, samples in ROUTE_SAMPLES.items():
        vecs = [embedder.embed_query(s) for s in samples]
        mat = l2_norm(np.array(vecs, dtype=float))
        idx[route] = mat
    return idx

ROUTE_INDEX = build_route_index(embedder)

def get_image_base64(image_bytes):
    encoded_string = base64.b64encode(image_bytes).decode('utf-8')
    return encoded_string


def route_semantic(q):
    q = (q or "").strip()
    if not q: return ("unknown", 0.0)
    qv = np.array(embedder.embed_query(q), dtype=float)
    qv = qv / (np.linalg.norm(qv) + 1e-12)
    best, score = "unknown", -1.0
    for name, mat in ROUTE_INDEX.items():
        s = float(mat.dot(qv).max()) if mat.size else -1.0
        if s > score:
            best, score = name, s
    if score < ROUTE_MIN_COS:
        return ("unknown", score)
    return (best, score)

def build_short_history(messages, k_turns=6, max_chars=1000):
    hist = []
    for m in messages[-2*k_turns:]:
        role = m["role"]
        hist.append(f"{role}:\n{m['content']}")
    return ("\n".join(hist))[-max_chars:] or "(blank)"

def reflection_rewrite(llm, history, last_q):
    short_hist = build_short_history(history, 6, 1000)
    sys = """You are a rewriting assistant for questions.
- Using the chat history and the latest user message, rewrite the LAST USER MESSAGE into a standalone Vietnamese version.
- If the last message contains MULTIPLE QUESTIONS, KEEP ALL of them. Do NOT drop, merge or reorder questions.
- Do NOT answer the questions.
- Output ONLY the rewritten message (no explanation). Max ~400 chars.
- If the last message is not a question, return it unchanged."""
    human = f"CHAT HISTORY (short):\n{short_hist}\n\nLATEST USER MESSAGE:\n{last_q}\n\nSTANDALONE QUESTION:"
    try:
        resp = llm.invoke([SystemMessage(content=sys), HumanMessage(content=human)])
        text = (resp.content or "").strip()
        return text if text else last_q
    except Exception:
        return last_q

def retrieve(col, embedder, query, subject_hint=None, k=FETCH_K):
    qvec = embedder.embed_query(query)
    where = {"subject": subject_hint} if subject_hint else {}
    res = col.query(query_embeddings=[qvec], n_results=k,
                    include=["metadatas","documents","distances"],
                    **({"where": where} if where else {}))
    docs  = res.get("documents", [[]])[0]
    metas = res.get("metadatas", [[]])[0]
    dists = res.get("distances", [[]])[0]
    return list(zip(docs, metas, dists))

def simple_rerank(q, cands, top_k=TOP_K):
    # d√πng reranker n·∫øu c√≥
    if reranker and cands:
        passages = [(d or "")[:1200] for (d,_,_) in cands]
        try:
            _, ranked_pass = reranker(q, passages)
            used = [False]*len(cands)
            out = []
            for p in ranked_pass:
                for i,(d,m,dist) in enumerate(cands):
                    if used[i]: continue
                    if (d or "")[:1200] == p:
                        out.append((d,m,dist)); used[i]=True; break
            return out[:min(top_k, len(out))]
        except Exception:
            pass
    # fallback heuristic
    qtok = set((q or "").lower().split())
    scored=[]
    for (d,m,dist) in cands:
        toks=set((d or "").lower().split())
        overlap=len(qtok & toks)
        dterm=0.0
        try: dterm=1/(1+float(dist))
        except: pass
        score=0.7*overlap+0.3*dterm
        scored.append((score,(d,m,dist)))
    scored.sort(key=lambda x:x[0], reverse=True)
    return [it for _,it in scored[:min(top_k,len(scored))]]

def build_context_guarded(ranked, max_chars=900):
    if not ranked: return ""
    good=[]
    for d,m,dist in ranked:
        try:
            if (dist is None) or (isinstance(dist,(int,float)) and dist<=DIST_THRES):
                good.append((d,m))
        except: continue
    if not good: return ""
    blocks=[]
    for d,m in good:
        head=f"[{m.get('subject','')}/{m.get('section','')}]"
        blocks.append(f"{head}\n{(d or '')[:max_chars]}")
    return "\n\n".join(blocks)

def is_compute_query(q, subject):
    ql=(q or "").lower()
    if subject and subject.lower()=="hedieuhanh":
        kws=["t√≠nh","waiting time","turnaround","response time","throughput","gantt",
             "fcfs","sjf","srtf","rr","round robin","quantum","burst","arrival"]
        return any(k in ql for k in kws) and bool(re.search(r"\d", ql))
    if subject and subject.lower()=="xulyanh":
        kws=["t√≠nh","t√≠ch ch·∫≠p","convolution","kernel","3x3","5x5","padding","stride",
             "sobel","prewitt","laplacian","gradient","magnitude","otsu","threshold"]
        looks_num=bool(re.search(r"\d", ql)) or ("[" in ql and "]" in ql)
        return any(k in ql for k in kws) and looks_num
    return False

# ========= STREAM HELPERS =========
def stream_answer(llm, messages):
    """Stream token-by-token into a string (fallback to non-stream)."""
    try:
        acc = ""
        for chunk in llm.stream(messages):
            token = getattr(chunk, "content", None)
            if token:
                acc += token
                yield acc
        if not acc:
            # fallback khi provider kh√¥ng stream
            resp = llm.invoke(messages)
            yield (resp.content or "")
    except Exception:
        resp = llm.invoke(messages)
        yield (resp.content or "")

# ========= BRANCHES (prompts) =========
def make_rag_messages(query, context, lang="Vietnamese (Ti·∫øng Vi·ªát)"):
    sys = f"""You are TungTomChat. Answer ONLY using context below.
- Do NOT use outside knowledge.
- If context is insufficient, say so.
- Answer in {lang}.
- Include at least one inline citation like [subject/section]."""
    human = f"QUESTION:\n{query}\n\nCONTEXT:\n{context}\n\nANSWER:"
    return [SystemMessage(content=sys), HumanMessage(content=human)]

def make_compute_messages(query, ctx_hint, lang="Vietnamese (Ti·∫øng Vi·ªát)"):
    sys = f"""B·∫°n l√† TungTomChat (Compute Agent).
- Gi·∫£i b√†i t·∫≠p t√≠nh to√°n k·ªπ thu·∫≠t b·∫±ng c√¥ng th·ª©c/thu·∫≠t to√°n chu·∫©n.
- Thi·∫øu tham s·ªë th√¨ LI·ªÜT K√ä v√† D·ª™NG, kh√¥ng b·ªãa.
- Tr√¨nh b√†y: (1) D·ªØ li·ªáu v√†o, (2) C√¥ng th·ª©c/Thu·∫≠t to√°n, (3) T√≠nh t·ª´ng b∆∞·ªõc, (4) K·∫øt qu·∫£.
- Tr·∫£ l·ªùi b·∫±ng {lang}."""
    human = f"C√¢u h·ªèi:\n{query}\n\nG·ª¢I √ù (n·∫øu c√≥):\n{(ctx_hint or '(tr·ªëng)')[:1500]}\n\nY√äU C·∫¶U: nh∆∞ h∆∞·ªõng d·∫´n tr√™n."
    return [SystemMessage(content=sys), HumanMessage(content=human)]

def answer_chitchat_with_tools(question, debug=False):
    sys = "You are TungtomChat, a friendly assistant. Answer concisely in Vietnamese."
    human = f"QUESTION:\n{question}\n\nAnswer:"
    llm_tools = llm.bind_tools(GENERAL_TOOLS)
    msgs = [SystemMessage(content=sys), HumanMessage(content=human)]
    first = llm_tools.invoke(msgs)
    logs=[]
    if getattr(first, "tool_calls", None):
        msgs.append(first)
        for tc in first.tool_calls:
            tool_name, tool_args = tc["name"], tc["args"]
            if debug: logs.append(f"‚Ü™ g·ªçi tool: {tool_name}({tool_args})")
            tool_obj = next((t for t in GENERAL_TOOLS if t.name == tool_name), None)
            if tool_obj:
                out = tool_obj.invoke(tool_args)
                msgs.append(ToolMessage(content=str(out), tool_call_id=tc["id"]))
                if debug: logs.append(f"‚Ü© tool tr·∫£ v·ªÅ: {out}")
            else:
                msgs.append(ToolMessage(content=f"Tool {tool_name} not found", tool_call_id=tc["id"]))
                if debug: logs.append(f"‚ö† tool kh√¥ng t·ªìn t·∫°i: {tool_name}")
        # t·ªïng h·ª£p cu·ªëi (stream)
        return msgs, logs
    else:
        # Kh√¥ng c·∫ßn tool ‚áí stream ngay n·ªôi dung first
        # fake messages ƒë·ªÉ stream helper d√πng l·∫°i 1 flow
        return [SystemMessage(content=sys), HumanMessage(content=human)], logs

# ========= UI =========
st.set_page_config(page_title="TungTomChat", page_icon="ü¶ê", layout="wide")
st.markdown("<style>.stChatMessage { font-size: 16px; }</style>", unsafe_allow_html=True)

with st.sidebar:
    st.title("ü¶ê TungTomChat")
    debug = st.toggle("Debug", value=False)
    with st.popover("T√πy ch·ªânh"):
        st.caption("Ch·ªâ b·∫≠t khi c·∫ßn:")
        DIST_THRES = st.slider("Distance threshold", 0.5, 2.5, DIST_THRES, 0.05)
        ROUTE_MIN_COS = st.slider("Route min cosine", 0.0, 0.9, ROUTE_MIN_COS, 0.01)
        FETCH_K = st.number_input("Fetch K", 5, 100, FETCH_K, 1)
        TOP_K = st.number_input("Top K", 1, 30, TOP_K, 1)

if "dialog" not in st.session_state:
    st.session_state.dialog = []

st.header("TungTomChat ‚Äî PTIT Study Assistant")

# render history
for m in st.session_state.dialog:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])


uploaded_file = st.file_uploader(
        "T·∫£i ·∫£nh l√™n",
        type = ["png", "jpg", "jpeg", "bmp", "gif"]
    )

if uploaded_file:
    st.image(uploaded_file.getvalue(),caption = "·∫¢nh v·ª´a t·∫£i l√™n",width = 200)


user_msg = st.chat_input("G√µ c√¢u h·ªèi c·ªßa b·∫°n‚Ä¶")
if user_msg:
    # 1) show user immediately
    st.session_state.dialog.append({"role":"user","content":user_msg})
    with st.chat_message("user"):
        st.markdown(user_msg)
        if uploaded_file:
            st.image(uploaded_file.getvalue(),width = 200)

    # 2) reflection (nh∆∞ng kh√¥ng show tr√†n; ch·ªâ show b·∫£n rewrite n·∫øu kh√°c)

    # 4) t·∫°o bong b√≥ng assistant + thinking placeholder
    with st.chat_message("assistant"):
        placeholder = st.empty()
        placeholder.markdown("*(ƒêang suy nghƒ©‚Ä¶)*")

        if uploaded_file:
            try:
                image_bytes = uploaded_file.getvalue()
                base64_image = get_image_base64(image_bytes)
                system_message = SystemMessage(
                    content = "B·∫°n l√† tr·ª£ l√Ω h·ªçc t·∫≠p AI. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch chi ti·∫øt, t·∫≠p trung v√†o n·ªôi dung h·ªçc thu·∫≠t. C√¢u tr·∫£ l·ªùi ph·∫£i d·ª±a tr√™n c·∫£ vƒÉn b·∫£n v√† H√åNH ·∫¢NH m√† ng∆∞·ªùi d√πng cung c·∫•p. N·∫øu ƒë√≥ l√† m·ªôt b√†i to√°n, h√£y gi·∫£i n√≥ t·ª´ng b∆∞·ªõc."
                )
                human_message = HumanMessage(
                    content = [
                        {"type":"text","text":user_msg},
                        {
                            "type":"image_url",
                            "image_url":{
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                )
                answer_text = ""
                for partial in stream_answer(llm,[system_message,human_message]):
                    answer_text = partial 
                    placeholder.markdown(answer_text)
                
                st.session_state.dialog.append({"role":"assistant","content":answer_text})
                st.stop()
            except Exception as e:
                placeholder.markdown(f"L·ªói khi x·ª≠ l√Ω ·∫£nh: {e}")
                st.stop()

        rewritten = reflection_rewrite(llm, st.session_state.dialog, user_msg)

    # 3) route (·∫©n m·∫∑c ƒë·ªãnh, ch·ªâ hi·ªán khi debug)
        route, conf = route_semantic(rewritten)
        if debug:
            st.caption(f"Route: **{route}** (conf={conf:.2f})")

        # === CHITCHAT ===
        if route.lower() == "chitchat":
            msgs, logs = answer_chitchat_with_tools(rewritten, debug=debug)
            # n·∫øu l√† case kh√¥ng tool, msgs l√† prompt; n·∫øu c√≥ tool, msgs ƒë√£ append ToolMessage s·∫µn ‚áí stream t·ªïng h·ª£p
            # stream
            answer_text = ""
            for partial in stream_answer(llm, msgs):
                answer_text = partial
                placeholder.markdown(answer_text)
            if debug and logs:
                st.caption("\n".join(logs))

            st.session_state.dialog.append({"role":"assistant","content":answer_text})
            st.stop()

        # === COMPUTE (OS/XLA c√≥ s·ªë li·ªáu) ===
        if route.lower() in ["hedieuhanh","xulyanh"] and is_compute_query(rewritten, route):
            cands = retrieve(collection, embedder, rewritten, subject_hint=route, k=FETCH_K)
            ranked = simple_rerank(rewritten, cands, top_k=TOP_K)
            ctx = build_context_guarded(ranked, 900)
            # stream compute
            answer_text = ""
            for partial in stream_answer(llm, make_compute_messages(rewritten, ctx)):
                answer_text = partial
                placeholder.markdown(answer_text)
            st.session_state.dialog.append({"role":"assistant","content":answer_text})
            if debug:
                with st.expander("Retrieve preview"):
                    for i,(d,m,dist) in enumerate(ranked,1):
                        st.write(f"[{i}] dist={dist:.3f} | {m.get('subject')}/{m.get('section')}")
                        st.caption((d or "")[:280]+"‚Ä¶")
                with st.expander("Context"):
                    st.code(ctx)
            st.stop()

        # === RAG L√ù THUY·∫æT / UNKNOWN ===
        subject_for_query = None if route.lower()=="unknown" else route
        cands = retrieve(collection, embedder, rewritten, subject_hint=subject_for_query, k=FETCH_K)
        ranked = simple_rerank(rewritten, cands, top_k=TOP_K)
        ctx = build_context_guarded(ranked, 900)

        # stream rag strict
        answer_text = ""
        for partial in stream_answer(llm, make_rag_messages(rewritten, ctx)):
            answer_text = partial
            placeholder.markdown(answer_text)

        st.session_state.dialog.append({"role":"assistant","content":answer_text})
        if debug:
            with st.expander("Retrieve preview"):
                for i,(d,m,dist) in enumerate(ranked,1):
                    st.write(f"[{i}] dist={dist:.3f} | {m.get('subject')}/{m.get('section')}")
                    st.caption((d or "")[:280]+"‚Ä¶")
            with st.expander("Context"):
                st.code(ctx)
